{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27c247f1-a99a-4375-8fa9-ebcf0ab44355",
   "metadata": {},
   "source": [
    "# AI-Powered CV Analysis for Career Guidance\n",
    "\n",
    "  ## Project Overview\n",
    "  This notebook demonstrates the development of an AI system designed to assist university students and unemployed graduates by:\n",
    "  1.  **Classifying Resumes:** Automatically categorizing CVs into professional fields.\n",
    "  2.  **Estimating Job Salary:** Providing a data-driven salary estimation for potential job roles.\n",
    "  3.  **Resume Professionalism Scoring:** Offering a benchmark for resume quality to help users fortify their CVs (a score above 0.6 is considered good).\n",
    "\n",
    "  **Author:** Jean-Paul Gergess and Kassem Chebly\n",
    "  **Date:** May 31, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4a0381-4a3a-4cda-9488-a720a9e3f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have these libraries installed in the Python environment\n",
    "  # that your Jupyter Notebook is using. If you get a ModuleNotFoundError,\n",
    "  # you might need to run: !pip install pandas tensorflow scikit-learn matplotlib seaborn\n",
    "\n",
    "  import tensorflow as tf\n",
    "  from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "  from tensorflow.keras.models import Sequential\n",
    "  from tensorflow.keras.layers import TextVectorization # For TF 2.6+\n",
    "  import numpy as np\n",
    "  import pandas as pd\n",
    "  from sklearn.model_selection import train_test_split\n",
    "  from sklearn.preprocessing import LabelEncoder\n",
    "  from sklearn.metrics import classification_report, confusion_matrix\n",
    "  import matplotlib.pyplot as plt\n",
    "  import seaborn as sns\n",
    "  import os\n",
    "  import re\n",
    "  import string\n",
    "  import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea14054e-a85a-4c42-ac79-55116dce338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "  MAX_FEATURES = 15000  # Max number of unique words in vocabulary for TextVectorization\n",
    "  SEQUENCE_LENGTH = 512 # Max length of a processed CV text sequence\n",
    "\n",
    "  # Define File Paths (ADJUST THESE BASED ON YOUR PROJECT FOLDER STRUCTURE)\n",
    "  RESUME_DATA_PATH = 'resume_dataset.csv'\n",
    "  SALARY_DATA_PATH = 'glassdoor_salaries.csv'\n",
    "  SAVE_DIR = 'salary_model_assets' # Directory to save models and assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b76662-26b4-4d63-aa23-39e572d59c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Loading Resume Dataset ---\")\n",
    "  resume_df = pd.read_csv(RESUME_DATA_PATH)\n",
    "  print(\"Dataset loaded successfully.\")\n",
    "  print(\"\\nResume Dataset Info:\")\n",
    "  resume_df.info()\n",
    "  print(\"\\nResume Categories Distribution:\")\n",
    "  print(resume_df['Category'].value_counts())\n",
    "  print(\"\\nFirst 5 rows of Resume Dataset:\")\n",
    "  print(resume_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a194c-d621-4793-a2e2-b0059b3eb628",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Loading Salary Dataset ---\")\n",
    "  salary_df = pd.read_csv(SALARY_DATA_PATH)\n",
    "  print(\"Salary dataset loaded successfully.\")\n",
    "  print(\"\\nSalary Dataset Info:\")\n",
    "  salary_df.info()\n",
    "  print(\"\\nFirst 5 rows of Salary Dataset:\")\n",
    "  print(salary_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99694d6-6c74-4b44-b7fe-a9bcb4d2a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "      # Convert to string and lowercase\n",
    "      text = str(text).lower()\n",
    "      # Remove punctuation\n",
    "      text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "      # Remove numbers\n",
    "      text = re.sub(r'\\d+', '', text)\n",
    "      # Remove extra whitespace and strip leading/trailing spaces\n",
    "      text = re.sub(r'\\s+', ' ', text).strip()\n",
    "      return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc61d4d-bbb4-4854-964b-f3acd220eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Applying Text Preprocessing ---\")\n",
    "  resume_df['Cleaned_Resume'] = resume_df['Resume'].apply(preprocess_text)\n",
    "  print(\"Text cleaning applied to 'Resume' column.\")\n",
    "  print(\"\\nFirst 5 rows with Cleaned_Resume:\")\n",
    "  print(resume_df[['Resume', 'Cleaned_Resume']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd83b6-b49e-4d84-a716-9b7b921e75d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Encoding Categories ---\")\n",
    "  label_encoder = LabelEncoder()\n",
    "  resume_df['Category_Encoded'] = label_encoder.fit_transform(resume_df['Category'])\n",
    "\n",
    "  # Save the LabelEncoder for future use (e.g., in an app.py script)\n",
    "  encoder_path = os.path.join(SAVE_DIR, 'label_encoder.pkl')\n",
    "  with open(encoder_path, 'wb') as f:\n",
    "      pickle.dump(label_encoder, f)\n",
    "  print(f\"LabelEncoder fitted and saved to {encoder_path}\")\n",
    "\n",
    "  print(\"\\nCategory Mappings:\")\n",
    "  for i, category in enumerate(label_encoder.classes_):\n",
    "      print(f\"{i}: {category}\")\n",
    "  print(\"\\nFirst 5 rows with Category_Encoded:\")\n",
    "  print(resume_df[['Category', 'Category_Encoded']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d41472-dec4-43ed-9db3-57cfe5770439",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Splitting Data into Training and Validation Sets ---\")\n",
    "  X = resume_df['Cleaned_Resume']\n",
    "  y = resume_df['Category_Encoded']\n",
    "\n",
    "  # Use stratify=y to ensure category distribution is similar in both splits\n",
    "  X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "  print(f\"Training samples: {len(X_train)}\")\n",
    "  print(f\"Validation samples: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b66b1f-26fd-4c44-8210-550edee54c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Adapting TextVectorization Layer ---\")\n",
    "  vectorize_layer = TextVectorization(\n",
    "      max_tokens=MAX_FEATURES,\n",
    "      output_mode='int', # Outputs integer indices for words\n",
    "      output_sequence_length=SEQUENCE_LENGTH # Pads/truncates sequences to this length\n",
    "  )\n",
    "  # Adapt the layer ONLY on the training data to prevent data leakage from validation set\n",
    "  vectorize_layer.adapt(X_train)\n",
    "  print(f\"Vocabulary size created by TextVectorization: {len(vectorize_layer.get_vocabulary())}\")\n",
    "\n",
    "  # Save the adapted TextVectorization layer so it can be loaded later for consistent preprocessing\n",
    "  vectorizer_model = tf.keras.models.Sequential([vectorize_layer]) # Wrap in Sequential to save it\n",
    "  vectorizer_model_path = os.path.join(SAVE_DIR, 'text_vectorizer_model.keras')\n",
    "  vectorizer_model.save(vectorizer_model_path)\n",
    "  print(f\"TextVectorization layer saved to {vectorizer_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3f777d-6c66-4fd5-8b7d-d0befd445881",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Applying Vectorization to Training and Validation Data ---\")\n",
    "  X_train_vectorized = vectorize_layer(X_train)\n",
    "  X_val_vectorized = vectorize_layer(X_val)\n",
    "  print(f\"Shape of vectorized training data: {X_train_vectorized.shape}\")\n",
    "  print(f\"Shape of vectorized validation data: {X_val_vectorized.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84dcb56-50ad-45ce-a70b-c1eff3fe9b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Building the Classification Model Architecture ---\")\n",
    "  model = Sequential([\n",
    "      # Embedding layer: Converts integer sequences to dense vectors. +1 for OOV token.\n",
    "      Embedding(input_dim=MAX_FEATURES + 1, output_dim=256, input_length=SEQUENCE_LENGTH),\n",
    "      # Bidirectional LSTM layers: Process sequences in both directions to capture more context.\n",
    "      Bidirectional(LSTM(128, return_sequences=True)), # return_sequences=True to stack another LSTM\n",
    "      Bidirectional(LSTM(64)), # This is the last LSTM layer, so no return_sequences\n",
    "      # Dense layers for feature learning\n",
    "      Dense(128, activation='relu'),\n",
    "      Dense(64, activation='relu'),\n",
    "      # Output layer: 'softmax' for multi-class classification, units = number of unique categories\n",
    "      Dense(len(label_encoder.classes_), activation='softmax')\n",
    "  ])\n",
    "  print(\"Model architecture defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362cea1d-a8ef-4493-83fc-96b03bf9c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Compiling the Model ---\")\n",
    "  model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy', # Use this for integer-encoded labels (like from LabelEncoder)\n",
    "                metrics=['accuracy'])\n",
    "  print(\"Model compiled successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc7b97-3fe8-4fec-89d1-3d8c4b0fabe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Model Summary ---\")\n",
    "  model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf0ce8-ec99-4af5-86a9-ba8a78025652",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Starting Model Training ---\")\n",
    "  # Callbacks for better training control\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "  model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "      filepath=os.path.join(SAVE_DIR, 'best_classification_model.keras'), # Saves in .keras format\n",
    "      monitor='val_accuracy', # Monitor validation accuracy\n",
    "      save_best_only=True,    # Save only the model with the best val_accuracy\n",
    "      verbose=1               # Log when a new best model is saved\n",
    "  )\n",
    "\n",
    "  history = model.fit(X_train_vectorized, y_train,\n",
    "                      epochs=20, # You can increase or decrease this based on performance and early stopping\n",
    "                      batch_size=32,\n",
    "                      validation_data=(X_val_vectorized, y_val),\n",
    "                      callbacks=[early_stopping, model_checkpoint])\n",
    "  print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390af702-7c4e-45cb-b13a-c7451abff55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Plotting Training History ---\")\n",
    "  plt.figure(figsize=(12, 5))\n",
    "\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "  plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "  plt.title('Training and Validation Accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.plot(history.history['loss'], label='Training Loss')\n",
    "  plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "  plt.title('Training and Validation Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5b58d8-9695-4db5-9c59-96b195c1b282",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Evaluating Model on Validation Set ---\")\n",
    "  loss, accuracy = model.evaluate(X_val_vectorized, y_val)\n",
    "  print(f\"Validation Loss: {loss:.4f}\")\n",
    "  print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e2834b-1245-4ff5-9bf6-d3faa2c34ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Generating Classification Report and Confusion Matrix ---\")\n",
    "  y_pred_probs = model.predict(X_val_vectorized)\n",
    "  y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "  # Convert encoded labels back to original category names for report\n",
    "  y_val_labels = label_encoder.inverse_transform(y_val)\n",
    "  y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "\n",
    "  print(\"\\nClassification Report:\")\n",
    "  print(classification_report(y_val_labels, y_pred_labels, target_names=label_encoder.classes_))\n",
    "\n",
    "  # Plot Confusion Matrix\n",
    "  cm = confusion_matrix(y_val_labels, y_pred_labels, labels=label_encoder.classes_) # Ensure labels order for consistent plotting\n",
    "  plt.figure(figsize=(12, 10))\n",
    "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "              xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "  plt.xlabel('Predicted Label')\n",
    "  plt.ylabel('True Label')\n",
    "  plt.title('Confusion Matrix for CV Classification')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c45609c-5ec6-44bd-9a5a-d403bc80bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Saving Final Model and Preprocessing Assets ---\")\n",
    "  # Save the final trained Keras model (if you didn't rely solely on ModelCheckpoint)\n",
    "  final_model_path = os.path.join(SAVE_DIR, 'final_classification_model.keras')\n",
    "  model.save(final_model_path)\n",
    "  print(f\"Final classification model saved to: {final_model_path}\")\n",
    "\n",
    "  # You've already saved label_encoder.pkl and text_vectorizer_model.keras earlier.\n",
    "  print(f\"LabelEncoder saved at: {os.path.join(SAVE_DIR, 'label_encoder.pkl')}\")\n",
    "  print(f\"TextVectorization layer saved at: {os.path.join(SAVE_DIR, 'text_vectorizer_model.keras')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc08593d-f2a2-40ea-85a1-a3fa4a2eb2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cv_texts = [\n",
    "      \"\"\"John Doe. Experienced Software Engineer with 5 years experience in web development using Python, Django, and React. Strong problem solver with expertise in AWS cloud services. Holds a M.Sc. in Computer Science from a top university. Seeking challenging roles in AI/ML engineering.\"\"\",\n",
    "      \"\"\"Jane Smith. Certified Public Accountant (CPA) with 7 years experience in corporate finance and auditing. Proficient in GAAP, IFRS, and SAP. Managed financial reporting for multinational companies. Excellent analytical and communication skills.\"\"\",\n",
    "      \"\"\"Fresh Graduate with a B.A. in Fine Arts, seeking entry-level graphic design position. Skills include Adobe Photoshop, Illustrator, and basic UX principles. Passionate about creative visual communication. Portfolio available upon request.\"\"\",\n",
    "      \"\"\"Michael Brown. Senior Project Manager with 10+ years experience in IT project delivery. PMP certified. Led cross-functional teams using Agile methodologies. Experience in software development lifecycle (SDLC). Strong leadership skills.\"\"\",\n",
    "      \"\"\"Ahmed Hassan. Dedicated customer service representative with 2 years experience in call center environments. Proficient in conflict resolution and CRM software. Strong verbal communication skills. Seeking growth opportunities.\"\"\",\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b4d29-36ae-41a2-b09c-95522f5a894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Making Predictions on New CVs and Assessing Professionalism ---\")\n",
    "  for i, cv_text in enumerate(new_cv_texts):\n",
    "      print(f\"\\n--- CV {i+1} ---\")\n",
    "      print(f\"Original (first 150 chars): '{cv_text[:150]}...'\")\n",
    "\n",
    "      # 1. Preprocess the new CV text\n",
    "      cleaned_text = preprocess_text(cv_text)\n",
    "      print(f\"Cleaned (first 100 chars): '{cleaned_text[:100]}...'\")\n",
    "\n",
    "      # 2. Vectorize the cleaned text using the loaded TextVectorization layer\n",
    "      vectorized_text = loaded_vectorize_layer([cleaned_text]) # Pass as a list for batch processing\n",
    "      print(f\"Vectorized shape: {vectorized_text.shape}\")\n",
    "\n",
    "      # 3. Make a prediction using the loaded model\n",
    "      predictions = loaded_model.predict(vectorized_text)\n",
    "      predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
    "      confidence = predictions[0, predicted_class_index]\n",
    "\n",
    "      # 4. Convert predicted index back to human-readable category\n",
    "      predicted_category = loaded_label_encoder.inverse_transform([predicted_class_index])[0]\n",
    "      print(f\"Predicted Category: **{predicted_category}** (Confidence: {confidence:.2f})\")\n",
    "\n",
    "      # 5. Implement Resume Professionalism Score (Conceptual)\n",
    "      # This is a conceptual implementation. In a real application, this score might\n",
    "      # involve deeper analysis (e.g., keyword density, specific skills matching,\n",
    "      # inferred experience level, or even a separate regression model for quality).\n",
    "      # For this project, we'll use prediction confidence as a simple proxy,\n",
    "      # assuming higher confidence in a predicted professional category implies more professionalism.\n",
    "      professionalism_score = confidence # Using confidence as the score for simplicity\n",
    "\n",
    "      print(f\"Resume Professionalism Score: {professionalism_score:.2f}\")\n",
    "\n",
    "      if professionalism_score >= 0.6: # Based on the problem statement's target\n",
    "          print(\"Status: **Pretty Good!** This resume appears professional and well-aligned with its category.\")\n",
    "      else:\n",
    "          print(\"Status: **Needs Fortification.** This resume might benefit from further improvements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b050777-3a91-4d85-9e1d-da627936ce70",
   "metadata": {},
   "source": [
    "## 9. Salary Estimation: Conceptual Integration\n",
    "\n",
    "  While the primary focus of this notebook was on robust CV classification, a crucial aspect of the overall project is the **salary estimation capability**. This component aims to provide job seekers with data-driven insights into potential earnings for specific roles, streamlining their application process.\n",
    "\n",
    "  ### 9.1 Approach for Salary Estimation\n",
    "\n",
    "  Integrating salary estimation into the project would typically involve the following steps and considerations:\n",
    "\n",
    "  1.  **Feature Engineering from CVs:**\n",
    "      * **Extraction of Structured Information:** The first step is to parse and extract quantifiable features from the cleaned CV text that are highly correlated with salary. These could include:\n",
    "          * **Years of Experience:** Identifying and calculating total professional experience from job history sections.\n",
    "          * **Key Skills:** Detecting mentions of specific, high-demand skills (e.g., Python, SQL, Cloud Computing, specific software platforms like SAP, Salesforce).\n",
    "          * **Education Level:** Categorizing degrees (Bachelor's, Master's, PhD) and fields of study.\n",
    "          * **Job Titles/Seniority:** Inferring seniority levels from job titles (e.g., \"Junior,\" \"Senior,\" \"Lead,\" \"Manager\").\n",
    "          * **Location:** If the CV contains location data, this is a significant factor.\n",
    "\n",
    "  2.  **Dedicated Salary Regression Model:**\n",
    "      * A separate machine learning model, or a regression head integrated into the deep learning model, would be trained specifically for salary prediction.\n",
    "      * This model would use the `glassdoor_salaries.csv` dataset (or a similar, richer salary dataset) for training. This dataset should contain features like years of experience, skills, education, job title, and the corresponding salary.\n",
    "      * Common regression model choices include Linear Regression, Ridge Regression, Random Forest Regressors, Gradient Boosting Machines (like LightGBM or XGBoost), or even a specialized neural network architecture for regression.\n",
    "\n",
    "  3.  **Prediction Flow for New CVs:**\n",
    "      * When a new CV is processed:\n",
    "          * It would first undergo the same text preprocessing and category classification as demonstrated in this notebook.\n",
    "          * Subsequently, the extracted features relevant to salary (as described above) would be fed as input to the pre-trained salary regression model.\n",
    "          * The output of this model would be an estimated salary (a continuous numerical value) or a predicted salary range.\n",
    "\n",
    "  ### 9.2 Example of Feature Extraction (Conceptual Code Placeholder)\n",
    "\n",
    "  ```python\n",
    "  # This is a conceptual function. Actual implementation would involve more complex NLP techniques\n",
    "  # (e.g., spaCy for NER, regex patterns for experience parsing, pre-defined skill lists).\n",
    "  def extract_salary_features(cleaned_cv_text, predicted_category):\n",
    "      features = {\n",
    "          'years_experience': 0, # Placeholder\n",
    "          'num_tech_skills': 0,  # Placeholder\n",
    "          'is_manager': 0,       # Placeholder\n",
    "          'is_degree_master_phd': 0, # Placeholder\n",
    "          'predicted_category': predicted_category\n",
    "      }\n",
    "\n",
    "      # Simple keyword-based example (for demonstration, not robust parsing)\n",
    "      if \"years experience\" in cleaned_cv_text:\n",
    "          match = re.search(r'(\\d+)\\s*years experience', cleaned_cv_text)\n",
    "          if match:\n",
    "              features['years_experience'] = int(match.group(1))\n",
    "\n",
    "      if \"python\" in cleaned_cv_text or \"java\" in cleaned_cv_text or \"aws\" in cleaned_cv_text:\n",
    "          features['num_tech_skills'] = 1 # A simple count\n",
    "\n",
    "      if \"manager\" in cleaned_cv_text or \"lead\" in cleaned_cv_text:\n",
    "          features['is_manager'] = 1\n",
    "\n",
    "      if \"m.sc.\" in cleaned_cv_text or \"phd\" in cleaned_cv_text:\n",
    "          features['is_degree_master_phd'] = 1\n",
    "\n",
    "      # In a full system, you would have a trained regression model here\n",
    "      # For demonstration, we'll use a very basic heuristic\n",
    "      base_salary = 50000\n",
    "      if features['years_experience'] > 5:\n",
    "          base_salary += features['years_experience'] * 5000\n",
    "      if features['is_manager']:\n",
    "          base_salary += 20000\n",
    "      if features['num_tech_skills'] > 0:\n",
    "          base_salary += 10000\n",
    "\n",
    "      # Adjust based on predicted category (example, not actual model)\n",
    "      if predicted_category == \"Software Developer\":\n",
    "          base_salary += 15000\n",
    "      elif predicted_category == \"HR\":\n",
    "          base_salary -= 10000\n",
    "      # ... and so on for other categories\n",
    "\n",
    "      return max(30000, base_salary) # Ensure a minimum salary\n",
    "\n",
    "  # Demonstrate conceptual salary estimation for a new CV\n",
    "  print(\"\\n--- Conceptual Salary Estimation for a New CV ---\")\n",
    "  sample_cv_text = new_cv_texts[0] # Take the first example CV\n",
    "  cleaned_sample_cv = preprocess_text(sample_cv_text)\n",
    "\n",
    "  # Re-predict category for this sample to ensure it's fresh\n",
    "  sample_vectorized = loaded_vectorize_layer([cleaned_sample_cv])\n",
    "  sample_predictions = loaded_model.predict(sample_vectorized)\n",
    "  sample_predicted_index = np.argmax(sample_predictions, axis=1)[0]\n",
    "  sample_predicted_category = loaded_label_encoder.inverse_transform([sample_predicted_index])[0]\n",
    "\n",
    "  estimated_salary = extract_salary_features(cleaned_sample_cv, sample_predicted_category)\n",
    "  print(f\"For the sample CV (Category: {sample_predicted_category}), Estimated Salary: ${estimated_salary:,.2f} (Conceptual)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ca3bb-e7b1-406d-b345-fe6da4de1660",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "  This project successfully developed an AI-powered system capable of classifying resumes into various professional categories, providing a significant step towards streamlining recruitment and empowering job seekers. The implementation of a robust text preprocessing pipeline and a Bidirectional LSTM-based deep learning model demonstrated effective learning from unstructured CV text. Furthermore, the project's scope included a crucial salary estimation component, allowing users to gain insight into potential earnings before applying. The iterative development process, marked by overcoming several technical challenges, underscored the importance of meticulous data preparation and careful model management in real-world ML applications.\n",
    "\n",
    "  The system successfully addresses the initial problem statement by offering both a resume classification and a conceptual framework for salary estimation, providing valuable tools for career guidance and resume optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a146ff-ed70-485c-9f66-bb0df0c2d9a5",
   "metadata": {},
   "source": [
    "## 11. Future Work and Enhancements\n",
    "\n",
    "  To further enhance this project and transition it into a robust production-ready tool, the following future work is recommended:\n",
    "\n",
    "  * **Full Salary Model Implementation:** Develop and rigorously train a dedicated regression model for salary estimation, incorporating more granular feature engineering (e.g., precise experience parsing, advanced skill weighting, geographical data if available). This would involve a more detailed analysis and modeling of the `glassdoor_salaries.csv` data.\n",
    "  * **Expand & Diversify Datasets:** Acquire larger and more diverse datasets for both CV classification and salary estimation to further improve model robustness, generalization, and fairness across various demographics and career paths.\n",
    "  * **Advanced Model Architectures:** Explore the application of state-of-the-art Transformer-based models (e.g., BERT, RoBERTa) for superior text understanding and potentially higher classification accuracy.\n",
    "  * **Explainable AI (XAI):** Implement techniques to provide transparency into model predictions, explaining *why* a CV was classified in a certain way or received a particular professionalism score.\n",
    "  * **Interactive User Interface:** Develop a user-friendly web application or API that allows job seekers to upload their CVs and receive instant classification, professionalism scores, and salary estimates.\n",
    "  * **Multi-task Learning Refinement:** Explore multi-task learning architectures where the classification and salary estimation tasks share common deep features, potentially leading to more efficient and accurate learning for both.\n",
    "  * **Continuous Integration/Deployment (CI/CD):** Set up automated pipelines for model retraining and deployment to ensure the system stays up-to-date with new data and improved models.\n",
    "\n",
    "  This project lays a strong foundation for an intelligent career guidance platform, with clear avenues for future development and enhancement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
